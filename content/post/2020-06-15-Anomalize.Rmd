---
title: "Website Time Series Anomaly Detection"
author: "Ed Orlando"
date: '2020-06-15'
output:
  html_document:
    df_print: paged
description: This blog walks viewers through how to utilize the anomalize package in R.
tags:
- anomalize
- tableau
- tidyverse
- readxl
- business science university
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    collapse = TRUE,
    out.width = "100%"
    )
```


Written by: Ed Orlando, Data Scientist
<br><br>

## Project Description
Anomaly detection can be valuable in many ways.  For example, anomalies can be used to detect fraud, better understand system health monitoring, or help data engineers identify spikes in website traffic.  It can also be used to remove extreme outliers from datasets before modeling.

In this project, webpage traffic outliers were identified that utilized data from the calendar year 2016.  The project focused on popular United States sports, entertainment, and political events and people.       

This project applied the **[anomalize](<https://business-science.github.io/anomalize/>)** package to assist in identifying outliers.
<br><br>

## Interactive Tableau Public Viz
A Tableau interactive viz was created that used the final output produced in this project.  The interactive viz can be viewed by clicking the link below.

**[Website Anomaly Detection App](<https://public.tableau.com/profile/ed.orlando#!/vizhome/WebsiteAnomalyDetection/WebsiteAnomalyDetection>)**.
<br><br>

## Load Libraries

To get started, install/load the libraries listed below.

```{r, echo=TRUE, results="hide", warning=FALSE, error=FALSE, message=FALSE}
# 1.0 LIBRARIES ----
library(vroom)
library(tidyverse)
library(tidyquant)
library(lubridate)
library(rsample)
library(anomalize)
library(fuzzyjoin)
library(readxl)
```

<br>

## Load Data

The original raw web data file included in the project can be downloaded **[here.](<https://www.kaggle.com/c/web-traffic-time-series-forecasting/data>)**  The raw data was modified in a prior project I have offline since the original raw data file was very large (271 MB).  I also created a couple lookup tables that create valuable features later in the pipeline.  

To follow along, click **[here](https://ed-orlando-07-blog-site.netlify.app/zip_files/Anomalize.zip)** to download the sample files.

```{r, warning=FALSE, error=FALSE, message=FALSE}
# 2.0 LOAD DATA ----
websites_sample_tbl <- vroom::vroom("Data_Sources/2020_06_15_Anomalize/websites_sample_tbl.csv", delim = ",")
page_summary_lkp_tbl <- read_xlsx("Data_Sources/2020_06_15_Anomalize/Topics_Dates_LKP.xlsx", sheet = "Sheet1")
topics_dates_lkp_tbl <- read_xlsx("Data_Sources/2020_06_15_Anomalize/Topics_Dates_LKP.xlsx", sheet = "Sheet2")
```
<br>

Subsets of the three (3) tibbles are listed below.  The tibbles include information related to website traffic visits, the topic summary, details about the outliers, and the sources and links to the details. 

```{r}
head(websites_sample_tbl)
```

```{r}
head(page_summary_lkp_tbl)
```

```{r}
head(topics_dates_lkp_tbl)
```

<br>
<br>

## Finding Anomalies

A few handy functions in the **[anomalize](<https://business-science.github.io/anomalize/>)** package were included in the analysis.  The official descriptions included in the site above are listed below.  

* time_decompose(): Separates the time series into seasonal, trend, and remainder components  
* anomalize(): Applies anomaly detection methods to the remainder component.  
* recomposed_l1 & recomposed_l2: added to calculate the limits that separate the “normal” data from the anomalies.

```{r}
## ANOMALY DETECTION -----
websites_anomalies_tbl <- websites_sample_tbl %>%
    group_by(Page_Summary) %>%
    filter(is.na(date) == FALSE) %>%
    time_decompose(visits, method = "stl") %>%
    anomalize(remainder, method = "iqr", alpha = 0.014) %>%
    mutate(recomposed_l1 = season + trend + remainder_l1) %>%
    mutate(recomposed_l2 = season + trend + remainder_l2)
```

```{r}
websites_anomalies_tbl %>% glimpse()
```

<br>

Finally, an example of the anomalies viewed in R is listed below. The anomalize package includes a plot_anomalies() function that easily identifies the outliers.

In 2016, the Chicago Cubs were in the playoffs and eventually won the World Series.  The popularity of that event is obviously apparent in the visual below. 
```{r}
websites_anomalies_tbl %>%
  filter(Page_Summary == "Chicago_Cubs") %>%
  filter(date >= as.Date("2016-01-01")) %>%
  plot_anomalies(ncol = 2, time_recomposed = TRUE)
```

To view the fully interactive Tableau Public viz with all 2016 popular topics listed, please click **[here.](<https://public.tableau.com/profile/ed.orlando#!/vizhome/WebsiteAnomalyDetection/WebsiteAnomalyDetection>)**

<br>
<br>

## Final Tidy Tibble With Proper Titles & Descriptions

Finally, we can add the web page Category, Proper Title, Description of Outlier Event, and the Source.  This tibble is what is exported and loaded into the Tableau Public viz.

```{r, warning=FALSE, error=FALSE, message=FALSE}
websites_tidy_tbl <- websites_anomalies_tbl %>%
    left_join(page_summary_lkp_tbl) %>%
    left_join(topics_dates_lkp_tbl)
```

```{r}
websites_tidy_tbl %>% glimpse()
```

<br>
For questions related to this analysis, please message me on **[LinkedIn](<https://www.linkedin.com/in/edorlando07/>)**.  

For access to more of my articles, please check out my **[blog](<https://ed-orlando-07-blog-site.netlify.app/>)**.
